# PBL 실습형 교재 문서 (PC–Pico 연동형)

## 1. 실습 주제

**웹캠 표정 인식 결과를 WS2812 매트릭스 이모지로 실시간 표현하기**

## 2. 프로젝트 개요

PC가 웹캠에서 얼굴을 추정·분류해 **감정 레이블(happy/sad/angry)** 을 MQTT로 발행합니다.
Pico는 해당 레이블을 받아 16×16 WS2812 매트릭스에 **간단한 이모지**를 그려 표시합니다.

## 3. 선행 작업 (작성 칸)

* (PC) 패키지 설치, 모델 파일 배치, GPU/CPU 설정
* (Pico) 네트워크 연결, 토픽/와이파이 설정
* (공통) 브로커 연결 테스트
  📸 (준비물/배선 사진 첨부 칸)

## 4. 개념 설명 — EmoAffectnet 간단 소개

사전 학습된 감정 인식 모델이 얼굴 이미지를 입력받아 **7개 클래스(Neutral, Happiness, Sadness, Surprise, Fear, Disgust, Anger)** 확률을 출력합니다.
본 예제는 MediaPipe로 얼굴 ROI를 추출한 뒤 TorchScript 모델로 추론하고, 결과를 **happy/sad/angry** 셋으로 매핑해 사용합니다.
Pico는 수신 레이블에 맞춰 **눈/입/장식 링** 등 픽셀 패턴을 조합해 이모지를 렌더링합니다.

---

## 5. PC 코드

```python
import cv2, math, numpy as np, time
import mediapipe as mp
import torch
from PIL import Image
from torchvision import transforms
import paho.mqtt.client as mqtt

BROKER="test.mosquitto.org"; PORT=1883; TOPIC="ticle/emo"

def pth_processing(fp):
    class PreprocessInput(torch.nn.Module):
        def __init__(self): super().__init__()
        def forward(self, x):
            x = x.to(torch.float32)
            x = torch.flip(x, dims=(0,))
            x[0, :, :] -= 91.4953; x[1, :, :] -= 103.8827; x[2, :, :] -= 131.0912
            return x
    ttransform = transforms.Compose([transforms.PILToTensor(), PreprocessInput()])
    img = fp.resize((224, 224), Image.Resampling.NEAREST)
    img = ttransform(img)
    return torch.unsqueeze(img, 0).to('cuda')

def norm_coordinates(nx, ny, w, h):
    x = min(math.floor(nx * w), w - 1); y = min(math.floor(ny * h), h - 1); return x, y

def get_box(fl, w, h):
    pts = [(norm_coordinates(l.x, l.y, w, h)) for l in fl.landmark]
    xs = np.asarray([p[0] for p in pts]); ys = np.asarray([p[1] for p in pts])
    x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()
    return max(0, x0), max(0, y0), min(w - 1, x1), min(h - 1, y1)

def display_EMO_PRED(img, box, label, lw=2):
    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
    cv2.rectangle(img, p1, p2, (255, 0, 255), lw, cv2.LINE_AA)
    cv2.putText(img, label, (p1[0], max(10, p1[1]-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,255), 2, cv2.LINE_AA)
    return img

def display_FPS(img, text):
    cv2.putText(img, text, (10, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)
    cv2.putText(img, text, (10, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)
    return img

mp_face_mesh = mp.solutions.face_mesh
name = '0_66_49_wo_gl'
pth_model = torch.jit.load(f'models_EmoAffectnet/torchscript_model_{name}.pth').to('cuda').eval()
DICT_EMO = {0:'Neutral',1:'Happiness',2:'Sadness',3:'Surprise',4:'Fear',5:'Disgust',6:'Anger'}

client = mqtt.Client(client_id="pc_emo_pub"); client.connect(BROKER, PORT, 60); client.loop_start()

cap = cv2.VideoCapture(0)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
last_sent = None; last_t = 0

with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
    while cap.isOpened():
        t1 = time.time()
        ok, frame = cap.read()
        if not ok: break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        res = face_mesh.process(rgb)

        if res.multi_face_landmarks:
            fl = res.multi_face_landmarks[0]
            x0,y0,x1,y1 = get_box(fl, w, h)
            face_roi = rgb[y0:y1, x0:x1]
            if face_roi.size > 0:
                inp = pth_processing(Image.fromarray(face_roi))
                prob = torch.nn.functional.softmax(pth_model(inp), dim=1).cpu().detach().numpy()[0]
                cls = int(np.argmax(prob)); label = DICT_EMO.get(cls, "Unknown")
                frame = display_EMO_PRED(frame, (x0,y0,x1,y1), label, 3)

                mapped = None
                if label=="Happiness": mapped="happy"
                elif label=="Sadness": mapped="sad"
                elif label=="Anger": mapped="angry"

                now = time.time()
                if mapped and (mapped!=last_sent or now-last_t>1.0):
                    client.publish(TOPIC, mapped.encode(), qos=0, retain=False)
                    last_sent = mapped; last_t = now

        fps = 1.0/max(1e-6, time.time()-t1)
        frame = display_FPS(frame, f'FPS: {fps:.1f}')
        cv2.imshow('Emo → MQTT', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'): break

cap.release(); cv2.destroyAllWindows()
client.loop_stop(); client.disconnect()
```

## 6. Pico 코드

```python
from ticle import WifiManager
from umqtt.simple2 import MQTTClient
from ws2812 import WS2812Matrix
import utime, math

W=16; H=16
m=WS2812Matrix([(3,1)])
m.clear()

Y=(255,200,0)
WHT=(255,255,255)
B=(0,160,255)
R=(255,70,70)
K=(0,0,0)

TOPIC=b"ticle/emo"
mode="happy"

cx=(W-1)/2; cy=(H-1)/2

def px(x,y,c):
    if 0<=x<W and 0<=y<H: m[x,y]=c

def clear():
    for y in range(H):
        for x in range(W): m[x,y]=K

def ring():
    for deg in range(0,360,2):
        r=7
        rad=math.radians(deg)
        x=int(cx+math.cos(rad)*r); y=int(cy+math.sin(rad)*r)
        px(x,y,Y)

def line(x0,y0,x1,y1,c):
    dx=abs(x1-x0); sx=1 if x0<x1 else -1
    dy=-abs(y1-y0); sy=1 if y0<y1 else -1
    err=dx+dy
    while True:
        px(x0,y0,c)
        if x0==x1 and y0==y1: break
        e2=2*err
        if e2>=dy: err+=dy; x0+=sx
        if e2<=dx: err+=dx; y0+=sy

def arc(points,c):
    for i in range(len(points)-1):
        x0,y0=points[i]; x1,y1=points[i+1]
        line(x0,y0,x1,y1,c)

def smile_curve(c):
    pts=[]
    for x in range(4,12):
        t=(x-cx)/4.0
        y=int(cy+2.5 - 2.0*(t*t)) + 2
        pts.append((x,y))
    arc(pts,c)

def frown_curve(c):
    pts=[]
    for x in range(4,12):
        t=(x-cx)/4.0
        y=int(cy-2.5 + 2.0*(t*t)) + 3
        pts.append((x,y))
    arc(pts,c)

def flat_curve(y,c):
    line(5,y,10,y,c)

def eyes_dots(c):
    px(5,6,c); px(10,6,c)

def eyes_slits(c):
    line(4,7,6,7,c); line(9,7,11,7,c)

def brows_angry(c):
    line(4,5,7,3,c); line(11,5,8,3,c)

def tears(c):
    line(5,7,5,12,c); line(10,7,10,12,c)

def emo_happy():
    clear(); ring(); eyes_dots(WHT); smile_curve(WHT)

def emo_sad():
    clear(); ring(); eyes_dots(WHT); tears(B); frown_curve(WHT)

def emo_angry():
    clear(); ring(); brows_angry(R); eyes_slits(WHT); flat_curve(11,R)

def draw():
    if mode=="happy": emo_happy()
    elif mode=="sad": emo_sad()
    else: emo_angry()
    m.update()

wifi=WifiManager()
wifi.connect("HBE_RSP","hanback91!")

def on_msg(t,msg,ret,dup):
    global mode
    s=msg.decode().strip().lower()
    if s in ("happy","sad","angry"):
        mode=s
        draw()

c=MQTTClient("pico_emo","test.mosquitto.org",port=1883)
c.set_callback(on_msg)
c.connect()
c.subscribe(TOPIC)

draw()

while True:
    c.check_msg()
    utime.sleep_ms(20)

```

---

## 7. 코드 요약 (핵심 로직만 코드블록으로 분리)

### 💻 PC 핵심 로직

#### 7-1) 얼굴 ROI 추출

```python
with mp.solutions.face_mesh.FaceMesh(max_num_faces=1, ...) as face_mesh:
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    res = face_mesh.process(rgb)
    if res.multi_face_landmarks:
        fl = res.multi_face_landmarks[0]
        x0,y0,x1,y1 = get_box(fl, w, h)
        face_roi = rgb[y0:y1, x0:x1]
```

#### 7-2) 전처리 → 모델 추론

```python
inp = pth_processing(Image.fromarray(face_roi))  # 224x224 + 채널 전처리
logits = pth_model(inp)
prob = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]
cls = int(np.argmax(prob))
label = DICT_EMO.get(cls, "Unknown")
```

#### 7-3) 레이블 매핑 → 발행(변화 시/주기적)

```python
mapped = "happy" if label=="Happiness" else "sad" if label=="Sadness" \
         else "angry" if label=="Anger" else None
if mapped and (mapped != last_sent or time.time()-last_t > 1.0):
    client.publish(TOPIC, mapped.encode(), qos=0, retain=False)
    last_sent, last_t = mapped, time.time()
```

#### 7-4) 시각 확인(박스/라벨/FPS)

```python
frame = display_EMO_PRED(frame, (x0,y0,x1,y1), label, 3)
frame = display_FPS(frame, f'FPS: {fps:.1f}')
```

---

### 🧠 Pico 핵심 로직

#### 7-5) 레이블 수신 → 즉시 렌더

```python
mode = "happy"

def on_msg(t, msg, ret, dup):
    global mode
    s = msg.decode().strip().lower()
    if s in ("happy","sad","angry"):
        mode = s
        draw()          # 현재 모드의 이모지 즉시 재그리기
```

#### 7-6) 이모지 프리셋(묶음 설명)

```python
def emo_happy():  clear(); ring(); eyes_dots(WHT);  smile_curve(WHT)
def emo_sad():    clear(); ring(); eyes_dots(WHT);  tears(B); frown_curve(WHT)
def emo_angry():  clear(); ring(); brows_angry(R);  eyes_slits(WHT); flat_curve(11,R)

def draw():
    (emo_happy if mode=="happy" else emo_sad if mode=="sad" else emo_angry)()
    m.update()
```

* `ring/eyes_*/brows_*/smile_curve/frown_curve/flat_curve` 등 **작은 도형 함수**를 조합해 표정을 구성합니다.

#### 7-7) 기본 도형(예시 2개)

```python
def line(x0,y0,x1,y1,c):
    dx=abs(x1-x0); sx=1 if x0<x1 else -1
    dy=-abs(y1-y0); sy=1 if y0<y1 else -1
    err=dx+dy
    while True:
        px(x0,y0,c)
        if x0==x1 and y0==y1: break
        e2=2*err
        if e2>=dy: err+=dy; x0+=sx
        if e2<=dx: err+=dx; y0+=sy

def smile_curve(c):
    pts=[]
    for x in range(4,12):
        t=(x-cx)/4.0
        y=int(cy+2.5 - 2.0*(t*t)) + 2
        pts.append((x,y))
    arc(pts,c)
```

---

## 8. 결과 확인

* PC 창에 얼굴 박스/감정 레이블·FPS가 표시됩니다.
* 매트릭스에는 **웃음/슬픔/화남** 이모지가 즉시 전환되어 출력됩니다.

📸 (PC 추론 화면 캡처 첨부 칸)
📸 (WS2812 매트릭스 이모지 사진 첨부 칸)

## 9. 연습문제

1. `surprise`, `neutral` 이모지를 추가해 5종으로 확장하세요.
2. 같은 레이블 연속 시 발행 주기를 2초로 제한해 네트워크 부하를 비교하세요.
3. 감정별 색상 팔레트를 바꿔 가독성을 높여 보세요.
